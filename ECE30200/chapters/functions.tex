\section{Functions}

\subsection{Functions of Random Variables}
In general, given some random variable
$X$, we may wish to know the properties
of $Y = g(X)$, where $g$ is a function.

To find the PDF of $Y = g(X)$, the first
step is to find the CDF
\begin{equation}
    F_Y(y) = F_X(g^{-1}(y))
\end{equation}
The next step is to find the PDF, given by
\begin{equation}
    f_Y(y) = \left(\frac{d}{dy}g^{-1}(y)\right)f_X(g^{-1}(y))
\end{equation}

Suppose $X$ is an exponential random variable
with parameter $\lambda$, and let $Y  = aX + b$.
Then the CDF and PDF of Y are respectively
\begin{align}
    F_Y(y) & = 1 - e^{-\frac{\lambda}{a}(y - b)}, y \geq b              \\
    f_Y(y) & = \frac{\lambda}{a}e^{-\frac{\lambda}{a}(y - b)}, y \geq b
\end{align}

Suppose $X$ is a uniform random variable in $\left[a, b\right], a > 0$,
and let $Y = X^2$. Then the CDF and PDF of Y are respectively
\begin{align}
    F_Y(y) & = \frac{\sqrt{y} - a}{b - a}, a^2 \leq y \leq b^2 \\
    f_Y(y) & = \frac{1}{\sqrt{y}(b - a)}, a^2 \leq y \leq b^2
\end{align}

To generate random numbers from an arbitrary
distribution $F_X$, first generate a random
number $U \sim Uniform(0,1)$, then let
$Y = F^{-1}_X(U)$. The distribution of $Y$ is
$F_X$.

Given two random variables $X$ and $Y$, the
PDF of $Z = X + Y$ is given by
\begin{align}
    f_Z(z) & = f_X(x) * f_Y(y)                          \\
           & = \int_{-\infty}^{\infty} f_X(z-y)f_Y(y)dy
\end{align}

As more random variablaes are summed, their
distribution (no matter the distribution) of each
individual variable) approaches a Gaussian.

Let $X_1 \sim \text{Gauss}(\mu_1, \sigma_1^2)$ and
$X_2 \sim \text{Gauss}(\mu_2, \sigma_2^2)$, then
\begin{equation}
    X_1 + X_2 \sim \text{Gauss}\left( \mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2 \right)
\end{equation}

Given two random variables $X$ and $Y$, the
PDF of $Z = XY$ is given by
\begin{equation}
    f_Z(z) = \int_{-\infty}^{\infty} \frac{1}{|y|} f_X(\frac{z}{y}) f_Y(y) dy
\end{equation}
The PDF of $Z = X - Y$ is given by
\begin{equation}
    f_Z(z) = \int_{-\infty}^{\infty} f_X(z + y) f_Y(y) \, dy
\end{equation}
The PDF of $Z = \frac{X}{Y}$ is given by
\begin{equation}
    f_Z(z) = \int_{-\infty}^{\infty} |y| \, f_X(z y) f_Y(y) \, dy
\end{equation}

For variables $X_1, X_2, \dots, X_n$, all
independent, let
\begin{equation}
    Z = \prod_{i = 1}^{n} X_i
\end{equation}
The density is given recursively
\begin{equation}
    f_Z(z) = \int_{-\infty}^{\infty} \frac{1}{|y|} f_W(\frac{z}{y}) f_{X_n}(y) dy
\end{equation}
where $W = \prod_{i=1}^{n-1}$ and $f_W$ is the
density of the product of the first $n - 1$ variables.

\subsection{Moment Generating Functions}
For any random variable $X$, the \emph{moment generating function}
(MGF) is
\begin{equation}
    M_X(s) = E\left[e^{sX}\right]
\end{equation}
For discrete $X$
\begin{equation}
    M_X(s) = \sum_{x\in \Omega} e^{sx} p_X(x)
\end{equation}
For continuous $X$
\begin{equation}
    M_X(s) = \int_{-\infty}^{\infty} e^{sx}f_X(x)dx
\end{equation}
MGFs have the following properties:
\begin{itemize}
    \item $M_X(0) = 1$ \\
    \item $\frac{d^k}{ds^k} M_X(s)\Big|_{s=0} = E\left[X^k\right], \quad k \in \mathcal{Z}^+$
\end{itemize}
The "moment generating" title comes
from the ability to determine any order
moment by evaluating the derivative at $s=0$.

Let $X$ and $Y$ be independent random variables.
Let $Z = X + Y$. Then
\begin{equation}
    M_Z(s) = M_X(s)M_Y(s)
\end{equation}
In general, let $Z = \sum_{n=1}^{N} X_n$. Then the
MGF of $Z$ is
\begin{equation}
    M_Z(s) = \prod_{n=1}^{N}M_{X_n}(s)
\end{equation}

Let $X_1, \dots, X_N$ be a sequence of i.i.d. Bernoulli
random variables with parameter $p$, then $Z = \sum_{i=1}^{N}X_i$
is a binomial random variable with parameters $(N, p)$.

\subsection{Characteristic Functions}
For this course, the \emph{characteristic function} of a
random variable $X$ is
\begin{equation}
    \Phi_X(j\omega) = E\left[e^{-j\omega X}\right]
\end{equation}