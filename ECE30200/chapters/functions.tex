\section{Functions}

\subsection{Functions of Random Variables}
In general, given some random variable
$X$, we may wish to know the properties
of $Y = g(X)$, where $g$ is a function.

To find the PDF of $Y = g(X)$, the first
step is to find the CDF
\begin{equation}
    F_Y(y) = F_X(g^{-1}(y))
\end{equation}
The next step is to find the PDF, given by
\begin{equation}
    f_Y(y) = \left(\frac{d}{dy}g^{-1}(y)\right)f_X(g^{-1}(y))
\end{equation}

Suppose $X$ is an exponential random variable
with parameter $\lambda$, and let $Y  = aX + b$.
Then the CDF and PDF of Y are respectively
\begin{align}
    F_Y(y) & = 1 - e^{-\frac{\lambda}{a}(y - b)}, y \geq b              \\
    f_Y(y) & = \frac{\lambda}{a}e^{-\frac{\lambda}{a}(y - b)}, y \geq b
\end{align}

Suppose $X$ is a uniform random variable in $\left[a, b\right], a > 0$,
and let $Y = X^2$. Then the CDF and PDF of Y are respectively
\begin{align}
    F_Y(y) & = \frac{\sqrt{y} - a}{b - a}, a^2 \leq y \leq b^2 \\
    f_Y(y) & = \frac{1}{\sqrt{y}(b - a)}, a^2 \leq y \leq b^2
\end{align}

To generate random numbers from an arbitrary
distribution $F_X$, first generate a random
number $U \sim Uniform(0,1)$, then let
$Y = F^{-1}_X(U)$. The distribution of $Y$ is
$F_X$.

Given two random variables $X$ and $Y$, the
PDF of $Z = X + Y$ is given by
\begin{align}
    f_Z(z) & = f_X(x) * f_Y(y)                          \\
           & = \int_{-\infty}^{\infty} f_X(z-y)f_Y(y)dy
\end{align}

As more random variablaes are summed, their
distribution (no matter the distribution) of each
individual variable) approaches a Gaussian.

Let $X_1 \sim \text{Gauss}(\mu_1, \sigma_1^2)$ and
$X_2 \sim \text{Gauss}(\mu_2, \sigma_2^2)$, then
\begin{equation}
    X_1 + X_2 \sim \text{Gauss}\left( \mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2 \right)
\end{equation}

Given two random variables $X$ and $Y$, the
PDF of $Z = XY$ is given by
\begin{equation}
    f_Z(z) = \int_{-\infty}^{\infty} \frac{1}{|y|} f_X(\frac{z}{y}) f_Y(y) dy
\end{equation}
The PDF of $Z = X - Y$ is given by
\begin{equation}
    f_Z(z) = \int_{-\infty}^{\infty} f_X(z + y) f_Y(y) \, dy
\end{equation}
The PDF of $Z = \frac{X}{Y}$ is given by
\begin{equation}
    f_Z(z) = \int_{-\infty}^{\infty} |y| \, f_X(z y) f_Y(y) \, dy
\end{equation}

For variables $X_1, X_2, \dots, X_n$, all
independent, let
\begin{equation}
    Z = \prod_{i = 1}^{n} X_i
\end{equation}
The density is given recursively
\begin{equation}
    f_Z(z) = \int_{-\infty}^{\infty} \frac{1}{|y|} f_W(\frac{z}{y}) f_{X_n}(y) dy
\end{equation}
where $W = \prod_{i=1}^{n-1}$ and $f_W$ is the
density of the product of the first $n - 1$ variables.

\subsection{Moment Generating Functions}
For any random variable $X$, the \emph{moment generating function}
(MGF) is
\begin{equation}
    M_X(s) = E\left[e^{sX}\right]
\end{equation}
For discrete $X$
\begin{equation}
    M_X(s) = \sum_{x\in \Omega} e^{sx} p_X(x)
\end{equation}
For continuous $X$
\begin{equation}
    M_X(s) = \int_{-\infty}^{\infty} e^{sx}f_X(x)dx
\end{equation}
MGFs have the following properties:
\begin{itemize}
    \item $M_X(0) = 1$ \\
    \item $\frac{d^k}{ds^k} M_X(s)\Big|_{s=0} = E\left[X^k\right], \quad k \in \mathcal{Z}^+$ \\
    \item $M_{aX}(s) = M_X(as)$
\end{itemize}
The "moment generating" title comes
from the ability to determine any order
moment by evaluating the derivative at $s=0$.

Let $X$ and $Y$ be independent random variables.
Let $Z = X + Y$. Then by the properties of exponents
\begin{equation}
    M_Z(s) = M_X(s)M_Y(s)
\end{equation}
In general, let $Z = \sum_{n=0}^{N} X_n$. Then the
MGF of $Z$ is
\begin{equation}
    M_Z(s) = \prod_{n=0}^{N}M_{X_n}(s)
\end{equation}

\begin{center}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Distribution} & \textbf{Parameter(s)}  & \textbf{MGF: $M_X(s) = E[e^{sX}]$}                            \\
        \hline
        Bernoulli             & $p$                    & $M_X(s) = (1-p) + pe^s$                                       \\
        \hline
        Binomial              & $n, p$                 & $M_X(s) = \left(1 - p + pe^s\right)^n$                        \\
        \hline
        Geometric             & $p$ (number of trials) & $M_X(s) = \frac{pe^s}{1 - (1 - p)e^s}, \quad s < -\ln(1 - p)$ \\
        \hline
        Poisson               & $\lambda$              & $M_X(s) = \exp\left(\lambda(e^s - 1)\right)$                  \\
        \hline
        Gaussian (Normal)     & $\mu, \sigma^2$        & $M_X(s) = \exp\left(\mu s + \frac{1}{2}\sigma^2 s^2\right)$   \\
        \hline
        Exponential           & $\lambda$              & $M_X(s) = \frac{\lambda}{\lambda - s}, \quad s < \lambda$     \\
        \hline
        Uniform               & $a, b$                 & $M_X(s) = \frac{e^{sb} - e^{sa}}{s(b - a)}, \quad s \ne 0$    \\
        \hline
    \end{tabular}
\end{center}

\subsection{Characteristic Functions}
For this course, the \emph{characteristic function} of a
random variable $X$ is
\begin{equation}
    \Phi_X(j\omega) = E\left[e^{-j\omega X}\right].
\end{equation}
Note that by this definition, the characteristic function
of a random variable is the same as its Fourier transform.
\begin{equation}
    \Phi_X(j\omega) = \mathcal{F}(X)
\end{equation}

\subsection{Autocorrelation Functions}
The \emph{autocorrelation function} of a
random process X(t) is
\begin{equation}
    R_X(t_1, t_2) = E\left[X(t_1)X(t_2)\right]
\end{equation}
for two time instants $t_1$ and $t_2$.

The \emph{cross-correlation function}
of $X(t)$ and $Y(t)$ is
\begin{equation}
    R_{X,Y}(t_1, t_2) = E[X(t_1)Y(t_2)]
\end{equation}
\subsection{Autocovariance Functions}
The \emph{autocovariance function} of a random
process $X(t)$ is
\begin{equation}
    C_X(t_1, t_2) = E\left[(X(t_1)-\mu_X(t_1))(X(t_2)-\mu_X(t_2))\right]
\end{equation}
Two useful properties are
\begin{itemize}
    \item $C_X(t_1, t_2) = R_X(t_1,t_2) - \mu_X(t_1)\mu_X(t_2)$
    \item $C_X(t, t) = \text{Var}(X(t))$
\end{itemize}

The \emph{cross-variance function} of
$X(t)$ and $Y(t)$ is
\begin{equation}
    C_{X,Y}(t_1, t_2) = E\left[(X(t_1)-\mu_X(t_1))(Y(t_2) - \mu_Y(t_2))\right]
\end{equation}

Note that if $\mu_X(t_1) = \mu_Y(t_2) = 0$, then
\begin{equation}
    C_{X,Y}(t_1, t_2) = R_{X,Y}(t_1, t_2)
\end{equation}