\section{Continuous Random Variables}

A \emph{continuous random variable} is
analogous to the discrete case. Recall that
a probability is just a size of a set.
It's easy to find the size of a discrete set
because you can just count elements, but for
an uncountable set new methods are needed. Luckily
the intution for continuous random variables is
intuitive, it's still just the size of a set $A$
relative to $\Omega$.
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[scale=1]
        \draw[thick] (0,0) circle (2cm);
        \node at (-1,0) {$\Omega$};
        \draw[thick, fill=gray!20] (0.7,0.5) circle (0.7cm);
        \node at (0.7,0.5) {$A$};
    \end{tikzpicture}
    \begin{tikzpicture}[scale=1]
        \draw[thick,->] (0,0) -- (5,0);
        \node[below] at (5,0) {$\Omega$};
        \draw[very thick,blue] (1,0) -- (3,0);
        \node[below,blue] at (2,0) {$A$};
        \draw (1,0.1) -- (1,-0.1);
        \draw (3,0.1) -- (3,-0.1);
    \end{tikzpicture}
    \caption{Continuous random variables}
\end{figure}
Formally, if each event in A is equally likely, then
\begin{equation}
    P[\{x \in A\}] = \frac{\int_{A}dx}{|\Omega|}
\end{equation}
If we relax the assumption of equiprobability, then
more generally
\begin{equation}
    P[\{x\in A\}] = \int_{A} f_X(x) dx
\end{equation}
$f_X(x)$ is called the \emph{probability density function} (PDF).
It is analogous to the probability mass function.

Formally, a probability density function
is a mapping $f_X: \Omega \implies \Re$,
with the following properties:
\begin{itemize}
    \item Non-negativity: $f_X(x) \geq 0 \forall x \in \Omega$
    \item Unity: $\int_{\Omega} f_X(x)dx = 1$
    \item Measure of a set: $P[\{x \in A\}] = \int_{A}f_X(x) dx$
\end{itemize}

We can express a PDF in terms of a PMF
with a train of delta functions like so:
\begin{equation}
    f_X(x) = \sum_{x_k \in \Omega} p_X(x_k) \delta(x - x_k)
\end{equation}

We can also define the probability density
function as the derivative of the CDF, like so:
\begin{equation}
    f_X(x) = \frac{d}{dx}p(X \leq x)
\end{equation}

The expectation of a continuous random variable is
\begin{equation}
    E[X] = \int_{\Omega} xf_X(x)dx
\end{equation}

Properties of the expectation for continuous
random variables:
\begin{itemize}
    \item $E[aX] = aE[X]$
    \item $E[X+a] = E[X] + a$
    \item $E[aX+b] = aE[X] + b$
\end{itemize}

A random variable $X$ has an expectation
if it is absolutely integrable,
\begin{equation}
    E[|X|] = \int_{\Omega} |x|f_X(x)dx < \infty
\end{equation}

The variance of a continuous random variable
$X$ is
\begin{align}
    Var[X] & = E[(X-\mu)^2]                    \\
           & = \int_{\Omega} (x-\mu)^2f_X(x)dx \\
           & = E[X^2] - \mu^2
\end{align}

A continuous \emph{uniform random variable}
has a PDF of
\begin{equation}
    f_X(x) = \begin{cases}
        \frac{1}{b-a} & a \leq x \leq b \\
        0             & \text{else}
    \end{cases}
\end{equation}
We write
\begin{equation}
    X \sim Uniform(a,b)
\end{equation}
to mean that $X$ is drawn from a uniform
distribution on an interval $[a, b]$.
It has a CDF given by
\begin{equation}
    F_X(x) = \begin{cases}
        0               & a < a           \\
        \frac{x-a}{b-a} & a \leq x \leq b \\
        1               & x > b
    \end{cases}
\end{equation}
If $X \sim Uniform(a,b)$ then
\begin{align}
    E[X]   & = \frac{a + b}{2}      \\
    Var[X] & = \frac{(b - a)^2}{12}
\end{align}

A continuous \emph{exponential random variable}
has a PDF of
\begin{equation}
    f_X(x) = \begin{cases}
        \lambda e^{-\lambda x} & x \geq 0    \\
        0                      & \text{else}
    \end{cases}
\end{equation}
\marginnote{An exponential random variable
    is the interarrival time between two consecutive
    Poisson events}
We write
\begin{equation}
    X \sim Exponential(\lambda)
\end{equation}
to mean that $X$ is drawn from an
exponential distribution of parameter
$\lambda$. It has a CDF given by
\begin{equation}
    F_X(x) = 1 - e^{-\lambda x}
\end{equation}
If $X \sim Exponential(\lambda)$, then
\begin{align}
    E[X]   & = \frac{1}{\lambda}   \\
    Var[X] & = \frac{1}{\lambda^2}
\end{align}

A \emph{Gaussian random variable} is a
random variable $X$ such that its PDF
is
\begin{equation}
    f_X(x) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\end{equation}
We write
\begin{equation}
    X \sim Gaussian(\mu, \sigma^2)
\end{equation}
or
\begin{equation}
    X \sim \mathcal{N}\left(\mu, \sigma^2\right)
\end{equation}
to mean that $X$ is drawn from a Gaussian
of parameter $(\mu, \sigma^2)$.
If $X \sim \mathcal{N}(\mu, \sigma^2)$, then
\begin{align}
    E[X]   & = \mu      \\
    Var[X] & = \sigma^2
\end{align}

The \emph{standard Gaussian} random variable has a
PDF given by
\begin{equation}
    f_X(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}
\end{equation}
The CDF of the standard Gaussian is defined
as the $\Phi$ function.
\begin{equation}
    \Phi(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{-\frac{t^2}{2}}dt
\end{equation}
The CDF of the standard Gaussian is related to the
\emph{error function}, which is defined as
\begin{equation}
    \text{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt
\end{equation}
by the relation
\begin{equation}
    \Phi(x) = \frac{1}{2} \left(1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right)
\end{equation}
The CDF of an arbitrary Gaussian is related via
the transformation
\begin{equation}
    F_X(x) = \phi\left(\frac{x - \mu}{\sigma}\right)
\end{equation}

In addition to mean and variance, we introduce
two more useful quantities, \emph{skewness} and
\emph{kurtosis}.
\begin{align}
    E\left[X\right]                                   & = \mu      \\
    E\left[(X - \mu)^2\right]                         & = \sigma^2 \\
    E\left[\left(\frac{X-\mu}{\sigma}\right)^3\right] & = \gamma   \\
    E\left[\left(\frac{X-\mu}{\sigma}\right)^4\right] & = \kappa
\end{align}
\marginnote{\emph{Excess kurtosis} is defined
    as $\kappa - 3$}

Skewness measures the asymmetry of a
distribution. A Gaussian distribution has
skewness 0. Kurtosis measures how heavy-tailed
the distribution is. If the kurtosis is
positive, then the tails decay faster than a
Gaussian. If the kurtosis is negative, then
the distribution has a tail that
decays more slowly than a Gaussian.

The definition of a CDF is
\begin{equation}
    F_X(x) = P[X \leq x]
\end{equation}
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{images/normal_distribution_plot.png}
    \caption{PDF and CDF}
\end{figure}
Let $X$ be a continuous random variable. if the CDF
$F_X$ is continuous at any $a\leq x \leq b$, then
\begin{equation}
    P[a \leq X \leq b] = F_X(b) - F_X(a)
\end{equation}
A function $F_X(x)$ is said to be left
continuous if at $x=b$
\begin{equation}
    F_X(b) - \lim_{h\implies 0} F_X(b-h)
\end{equation}
and right continuous if
\begin{equation}
    F_X(b) - \lim_{h\implies 0} F_X(b+h)
\end{equation}
and continuous if $F_X(x)$ is both left
and right continuous.
All CDFs are right continuous.

For any random variable $X$, discrete or continuous,
\begin{equation}
    P[X=b] = \begin{cases}
        F_X(b) - F_X(b^-) & \text{if $F_X$ is discontinuous at $x=b$} \\
        0                 & \text{else}
    \end{cases}
\end{equation}

The PDf is the derivative of the CDF.
\begin{equation}
    f_X(x) = \frac{d}{dx} \int_{-\infty}^{x} f_X(t)dt
\end{equation}
provided $F_X$ is differentiable at $x$. If not, then
\begin{equation}
    f_X(x) = F_X(x) - \lim_{h\implies 0} F_X(x-h)
\end{equation}

Let $X$ be a continuous random variable with PDF
$f_X$. The median of $X$ is a point $c \in \Re$ such that
\begin{equation}
    \int_{-\infty}^{c} f_X(x) dx = \int_{c}^{\infty}f_X(x) dx
\end{equation}

Let $X$ be a continuous random variable. The mode is
the point $c$ such that $f_X(x)$ attains the maximum.
\begin{equation}
    x = \text{argmax}_{x \in \Omega} f_X(x)
\end{equation}

The mean $E[X]$ can be computed from
$F_X$ as
\begin{equation}
    E[X] = \int_{0}^{\infty} (1-F_X(t))dt
\end{equation}