\section{Signed Arithmetic}

Bits in memory don't inherently represent anything.
What a given value in memory means depends on how
it is interpreted: signed, unsigned, floating point,
little-endian, big-endian.

\subsection{Basic Arithmetic}

The most common implementation of signed integers
is 2's complement, because addition is the same
for signed and unsigned binary numbers. Given a binary number
\texttt{\{$b^{31}$ $b^{30}$ ... $b^1$ $b^0$\}}.
If interpreted as an unsigned int, the value is
\begin{equation}
    b^{31} \times 2^{31} + b^{30} \times 2^{30} + \dots + b^{1} \times 2^{1} + b^{0} \times 2^{0}.
\end{equation}
The maximum is \texttt{11...11} ($2^{32} - 1$), and the minimum
is \texttt{00...00} (0).
If we instead interpret it as a signed integer, the value is
\begin{equation}
    -b^{31} \times 2^{31} + b^{30} \times 2^{30} + \dots + b^{1} \times 2^{1} + b^{0} \times 2^{0}.
\end{equation}
This means the maximum is \texttt{01...11} ($2^{31} - 1$),
and the minimum is \texttt{10...00} ($-2^{31}$).

To negate a 2's complement number, invert all bits and
add 1. To convert an $n$ bit number into a number with more than
$n$ bits, copy (sign extend) the MSB (the sign bit) into the other bits.
For example, \texttt{1001} becomes \texttt{11111001}.
\marginnote{Take a moment to justify to yourself why the negation
    and extension operations work.}

An ALU is shown in Figure \ref{fig:alu}.
\begin{figure}
    \includegraphics{images/alu.png}
    \caption{ALU}
    \label{fig:alu}
\end{figure}
Its structure reveals that for every number,
in fact the AND, OR, sum, and difference are
calculated, and a mux selects the results based
on the type of operation.

Overflow can occur when two positive numbers are
added, or two negative numbers are added. Overflow
can be detected with the XOR of the carry into MSB
and carry out of MSB.
\marginnote{No carry out is equivalent to a carry out of 0.}

Negative numbers and zero are required
for conditional branches, e.g. \texttt{beq}
and \texttt{blt}.

\subsection{Integer Addition}

Recall implementing an adder in hardware with a ripple-carry
adder, which performs addition the same way that humans do
with pencil and paper. Starting at the least significant digit position,
the two corresponding digits are added and a result is obtained.
A "carry out" may occur if the result requires a higher digit;
for example, "9 + 5 = 4, carry 1". Binary arithmetic works in the same
fashion, with fewer digits. In this case, there are only four possible
operations, 0+0, 0+1, 1+0 and 1+1; the 1+1 case generates a carry.
Accordingly, all digit positions other than the rightmost one need to
wait on the possibility of having to add an extra 1 from a carry on the
digits one position to the right. In practice ripple-carry addition is
too slow, so a \emph{carry-lookahead adder} (CLA)
\marginnote{Carry-lookahead adders are also known as fast adders.}
is used. The improvement in speed comes from the titular
looking ahead to the carry, wherein the CLA calculates one or more
carry bits before the sum and so reduces the wait time to calculate
the result of the larger-value bits of the adder.
\marginnote{There are many faster adders, including the carry-select adder,
    Han-Carlson adder, and the Ling Adder, among others.}

A CLA performs two operations in parallel:
\begin{itemize}
    \item Calculating for each digit position whether that position is going to propagate a carry if one comes in from the right.
    \item Combining these calculated values to be able to deduce quickly whether, for each group of digits, that group is going
          to propagate a carry that comes in from the right.
\end{itemize}
Supposing that groups of four digits are chosen. The sequence of events would go like this:
\begin{itemize}
    \item All 1-bit adders calculate their results. Simultaneously, the lookahead units perform their calculations.
    \item Assuming that a carry arises in a particular group, that carry will emerge at the left-hand end of the
          group within at most five gate delays and start propagating through the group to its left.
    \item If that carry is going to propagate all the way through the next group, the lookahead unit will already
          have deduced this. Accordingly, before the carry emerges from the next group, the lookahead unit is immediately
          (within one gate delay) able to tell the next group to the left that it is going to receive a carry, and at
          the same time, to tell the next lookahead unit to the left that a carry is on its way.
\end{itemize}
The net effect is that the carries start by propagating slowly through each 4-bit group, just as in a ripple-carry system,
but then move four times as fast, leaping from one lookahead-carry unit to the next. Finally, within each group that
receives a carry, the carry propagates slowly within the digits in that group.

The more bits in a group, the more complex the lookahead carry logic becomes, and the more time is spent on the "slow roads"
in each group rather than on the "fast road" between the groups (provided by the lookahead carry logic). On the other hand,
the fewer bits there are in a group, the more groups have to be traversed to get from one end of a number to the other, and
the less acceleration is obtained as a result.

\subsection{Integer Multiplication}

Recall that shifting left means propagating zeros in from the right.
When shifting right, there is both arithmetic shift (filling the
left with whatever the value of the MSB is) and logical shift (always
filling the left in with zeros). Shifting left corresponds to multiplication
by two, and shifting right corresponds to dividing by two.

Recall the algorithm for multiplying taught in grade school, which
multiplies each digit in one number by the other number and then
sums it all together.

The hardware implementation of this grade school multiplication algorithm
is in Figure \ref{fig:naivemult}.
\begin{figure}
    \includegraphics{images/naivemult.png}
    \caption{Naive Multiplication Hardware}
    \label{fig:naivemult}
\end{figure}
\begin{figure}
    \includegraphics{images/naivemultflowchart.png}
    \caption{Naive Multiplication Flowchart}
    \label{fig:naivemultflowchart}
\end{figure}
The case of $13 \times 11$ is show in Figure \ref{fig:naivemultexample}.
\begin{figure}
    \includegraphics{images/naivemultexample.png}
    \caption{Naive Multiplication Example}
    \label{fig:naivemultexample}
\end{figure}

This naive implementation can be improved a number of ways. One such
improvement is shown in Figure \ref{fig:mult2}.
\begin{figure}
    \includegraphics{images/mult2.png}
    \caption{Alternate Multiplication Hardware}
    \label{fig:mult2}
\end{figure}
\begin{figure}
    \includegraphics{images/mult2flowchart.png}
    \caption{Alternate Multiplication Flowchart}
    \label{fig:mult2flowchart}
\end{figure}
The case of $13 \times 11$ is show in Figure \ref{fig:naivemultexample}.
\begin{figure}
    \includegraphics{images/mult2example.png}
    \caption{Alternate Multiplication Example}
    \label{fig:mult2example}
\end{figure}

There are even more sophisticated and efficient implementations.

\subsection{Integer Division}

\subsection{Floating Point Arithmetic}