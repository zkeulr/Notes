\section{Asymptotic Notation}

\subsection{Big O}
Big \( O \) notation describes the upper bound 
of an algorithm's growth rate. It provides an 
asymptotic measure of the time complexity or 
space complexity of an algorithm as a function of 
the input size. Informally, big \( O \) notation
just picks out the highest 
order term from your runtime expression. 
Formally, Big \( O \) is defined as follows:
\[
f(n) \in O(g(n)) \iff \exists \, c > 0 \text{ and } n_0 \geq 0 \text{ such that } \forall n \geq n_0, \, 0 \leq f(n) \leq c \cdot g(n).
\]
\begin{itemize}
    \item \( f(n) \): The function representing the actual growth rate of the algorithm.
    \item \( g(n) \): The comparison function, often representing the dominant term of \( f(n) \).
    \item \( c > 0 \): A constant that scales \( g(n) \) to bound \( f(n) \).
    \item \( n_0 \): A constant representing the threshold beyond which the inequality holds.
\end{itemize}

Big \( O \) notation focuses on the dominant term of \( g(n) \), 
ignoring lower-order terms and constant factors, to provide a 
simplified representation of the algorithm's efficiency.

% \subsection{Big $\Theta$}